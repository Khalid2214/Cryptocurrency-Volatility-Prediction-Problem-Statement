{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPBuWj640mk63tsI1+SeV7/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["crypto_vol_project/\n","├── data/\n","│   └── dataset.csv                # provided by user\n","├── src/\n","│   ├── data_processing.py         # load & clean\n","│   ├── feature_engineering.py     # create features\n","│   ├── model_training.py          # training + hyperparam tuning\n","│   ├── eval_reports.py            # evaluation metrics & plots\n","│   └── utils.py                   # helper functions\n","├── deploy/\n","│   └── app_streamlit.py           # Streamlit app for local deployment\n","├── notebooks/\n","│   └── eda.ipynb                  # optional EDA notebook\n","├── requirements.txt\n","├── README.md\n","└── docs/\n","    ├── HLD.md\n","    └── LLD.md"],"metadata":{"id":"Junzq9B101T3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["requirements.txt\n","\n","pandas\n","numpy\n","scikit-learn\n","xgboost\n","lightgbm\n","matplotlib\n","seaborn\n","streamlit\n","joblib\n","pmdarima\n","statsmodels\n","scipy\n","optuna"],"metadata":{"id":"wHkIqCAi-WnO"}},{"cell_type":"code","source":["# src/utils.py\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import joblib\n","\n","\n","def load_dataset(path: str) -> pd.DataFrame:\n","    \"\"\"Load dataset from CSV path.\"\"\"\n","    df = pd.read_csv(path, parse_dates=True)\n","    return df\n","\n","\n","def save_model(model, path: str):\n","    joblib.dump(model, path)\n","\n","\n","def load_model(path: str):\n","    return joblib.load(path)\n","\n","\n","def train_test_split_time_series(X, y, test_size: float = 0.2):\n","    \"\"\"\n","    Simple time-aware split: keep last `test_size` portion as test.\n","    Assumes data is sorted by time index.\n","    \"\"\"\n","    n = len(X)\n","    split_idx = int(n * (1 - test_size))\n","    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n","    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n","    return X_train, X_test, y_train, y_test"],"metadata":{"id":"ZWROIOVe-X94","executionInfo":{"status":"ok","timestamp":1754745405001,"user_tz":-330,"elapsed":3723,"user":{"displayName":"Khalid Shabir","userId":"06714934434760757081"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# src/data_processing.py\n","import pandas as pd\n","import numpy as np\n","from src.utils import load_dataset\n","\n","\n","def basic_cleaning(df: pd.DataFrame) -> pd.DataFrame:\n","    \"\"\"Basic cleaning: parse dates, sort, handle missing values.\"\"\"\n","    # assume there's a 'timestamp' or 'date' column\n","    date_cols = [c for c in df.columns if 'time' in c.lower() or 'date' in c.lower()]\n","    if date_cols:\n","        df['timestamp'] = pd.to_datetime(df[date_cols[0]])\n","    else:\n","        # if no date column, try index or raise\n","        if not np.issubdtype(df.index.dtype, np.datetime64):\n","            raise ValueError('No date-like column found; ensure dataset has a timestamp/date column')\n","    df = df.sort_values('timestamp').reset_index(drop=True)\n","\n","    # forward-fill or interpolate missing numeric data\n","    num_cols = df.select_dtypes(include='number').columns.tolist()\n","    df[num_cols] = df[num_cols].interpolate().ffill().bfill()\n","\n","    return df\n","\n","\n","if __name__ == '__main__':\n","    df = load_dataset('/mnt/data/dataset.csv')\n","    df_clean = basic_cleaning(df)\n","    df_clean.to_csv('data/dataset_clean.csv', index=False)\n","    print('Saved cleaned dataset to data/dataset_clean.csv')"],"metadata":{"id":"-zhvo2TBAGgF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# src/feature_engineering.py\n","import pandas as pd\n","import numpy as np\n","\n","\n","def add_return_features(df: pd.DataFrame, price_col: str = 'close') -> pd.DataFrame:\n","    \"\"\"Add log returns, simple returns, rolling vol, and lag features.\"\"\"\n","    df = df.copy()\n","    # log return\n","    df['log_return'] = np.log(df[price_col]) - np.log(df[price_col].shift(1))\n","    # simple return\n","    df['ret_1'] = df[price_col].pct_change()\n","\n","    # rolling volatility (std of returns) - target candidate\n","    df['rolling_vol_7'] = df['log_return'].rolling(window=7).std()\n","    df['rolling_vol_21'] = df['log_return'].rolling(window=21).std()\n","\n","    # lags of returns\n","    for lag in [1,2,3,5,7]:\n","        df[f'logret_lag_{lag}'] = df['log_return'].shift(lag)\n","\n","    # moving averages\n","    df['ma_7'] = df[price_col].rolling(window=7).mean()\n","    df['ma_21'] = df[price_col].rolling(window=21).mean()\n","    df['ma_ratio'] = df['ma_7'] / (df['ma_21'] + 1e-9)\n","\n","    # Drop rows with NaNs due to shifting/rolling\n","    df = df.dropna().reset_index(drop=True)\n","    return df\n","\n","\n","if __name__ == '__main__':\n","    import sys\n","    path = sys.argv[1] if len(sys.argv) > 1 else 'data/dataset_clean.csv'\n","    df = pd.read_csv(path, parse_dates=['timestamp'])\n","    df_fe = add_return_features(df, price_col='close')\n","    df_fe.to_csv('data/dataset_fe.csv', index=False)\n","    print('Saved feature-engineered dataset to data/dataset_fe.csv')"],"metadata":{"id":"PmspXMvu-h8q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# src/model_training.py\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error, r2_score\n","import joblib\n","\n","from src.utils import train_test_split_time_series\n","\n","\n","def train_model(df_path='data/dataset_fe.csv', target='rolling_vol_7', model_out='models/rf_vol_model.joblib'):\n","    df = pd.read_csv(df_path, parse_dates=['timestamp'])\n","    features = [c for c in df.columns if c not in ['timestamp', target]]\n","    X = df[features]\n","    y = df[target]\n","\n","    X_train, X_test, y_train, y_test = train_test_split_time_series(X, y, test_size=0.2)\n","\n","    # baseline model\n","    rf = RandomForestRegressor(n_jobs=-1, random_state=42)\n","\n","    # time-series friendly CV\n","    tscv = TimeSeriesSplit(n_splits=5)\n","\n","    param_dist = {\n","        'n_estimators': [50, 100, 200, 400],\n","        'max_depth': [3, 5, 8, 12, None],\n","        'min_samples_split': [2, 5, 10],\n","        'min_samples_leaf': [1, 2, 4]\n","    }\n","\n","    rsearch = RandomizedSearchCV(rf, param_distributions=param_dist, n_iter=20,\n","                                 cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1, random_state=42, verbose=1)\n","    rsearch.fit(X_train, y_train)\n","\n","    best = rsearch.best_estimator_\n","    preds = best.predict(X_test)\n","\n","    mse = mean_squared_error(y_test, preds)\n","    rmse = np.sqrt(mse)\n","    r2 = r2_score(y_test, preds)\n","\n","    print('Best params:', rsearch.best_params_)\n","    print('Test RMSE:', rmse)\n","    print('Test R2:', r2)\n","\n","    # save model and optionally the search\n","    joblib.dump(best, model_out)\n","    joblib.dump(rsearch, model_out.replace('.joblib', '_search.joblib'))\n","\n","    # save an evaluation CSV\n","    pd.DataFrame({'y_true': y_test.values, 'y_pred': preds}).to_csv('models/eval_results.csv', index=False)\n","\n","    return best, {'rmse': rmse, 'r2': r2}\n","\n","\n","if __name__ == '__main__':\n","    import os\n","    os.makedirs('models', exist_ok=True)\n","    train_model()"],"metadata":{"id":"OyGOfaFM-pow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# src/eval_reports.py\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import mean_squared_error, r2_score\n","\n","\n","def plot_preds(eval_csv='models/eval_results.csv'):\n","    df = pd.read_csv(eval_csv)\n","    plt.figure(figsize=(10,4))\n","    plt.plot(df['y_true'].values, label='True')\n","    plt.plot(df['y_pred'].values, label='Pred')\n","    plt.legend()\n","    plt.title('True vs Predicted Volatility (Test set)')\n","    plt.savefig('models/plot_true_vs_pred.png', dpi=150)\n","    print('Saved plot to models/plot_true_vs_pred.png')\n","\n","\n","if __name__ == '__main__':\n","    plot_preds()"],"metadata":{"id":"FHBk1gkG-sbR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# deploy/app_streamlit.py\n","import streamlit as st\n","import pandas as pd\n","import numpy as np\n","import joblib\n","from src.feature_engineering import add_return_features\n","\n","MODEL_PATH = 'models/rf_vol_model.joblib'\n","\n","st.title('Crypto Volatility Predictor')\n","st.markdown('Upload CSV (ohclv) — columns should include timestamp and close price')\n","\n","uploaded = st.file_uploader('Choose a CSV', type=['csv'])\n","if uploaded is not None:\n","    df = pd.read_csv(uploaded, parse_dates=True)\n","    st.write('Raw preview:')\n","    st.dataframe(df.head())\n","\n","    # basic FE\n","    try:\n","        df_fe = add_return_features(df, price_col='close')\n","    except Exception as e:\n","        st.error(f'Feature engineering failed: {e}')\n","    else:\n","        st.write('Feature preview:')\n","        st.dataframe(df_fe.head())\n","\n","        # load model\n","        model = joblib.load(MODEL_PATH)\n","        features = [c for c in df_fe.columns if c not in ['timestamp', 'rolling_vol_7', 'rolling_vol_21']]\n","        X = df_fe[features]\n","        preds = model.predict(X)\n","        df_fe['pred_vol'] = preds\n","\n","        st.line_chart(df_fe.set_index('timestamp')[['rolling_vol_7', 'pred_vol']].tail(200))\n","        st.download_button('Download predictions CSV', df_fe.to_csv(index=False), file_name='predi.csv')"],"metadata":{"id":"3pg4qcla-4yl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install -r requirements.txt\n","streamlit run deploy/app_streamlit.py"],"metadata":{"id":"pjsxH_DoABil"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ip install -r requirements.txt\n","streamlit run deploy/app_streamlit.py"],"metadata":{"id":"B5WGag2m--Yp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Final Report (guidance)\n","\n","Include: dataset description, key EDA plots (trend, distribution, correlation heatmap), description of features, model selection rationale, hyperparameter tuning results, final metrics table, top failure cases and limitations, and suggestions for next steps (ensemble, volatility regimes, more exogenous features like orderbook or sentiment)."],"metadata":{"id":"u3jsv0O2_n0o"}},{"cell_type":"markdown","source":["How to run the pipeline quickly"],"metadata":{"id":"8uI_14Yh_sT8"}},{"cell_type":"code","source":["#1.Clean data:\n","python src/data_processing.py"],"metadata":{"id":"HLHx2tNF_9yb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2Feature engineering:\n","\n","python src/featu\n"],"metadata":{"id":"AWuyzQTt_ojw"},"execution_count":null,"outputs":[]}]}